---
title: "⭐🚩 PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents"
collection: publications
category: arxiv
permalink: /publication/2024-06-20-pin
excerpt: 'Knowledge‑intensive, Paired & Interleaved, Large Multimodal Models (LMMs), Scalability'
date: 2024-06-20
venue: 'Arxiv'
paperurl: 'https://arxiv.org/abs/2406.13923'
bibtexurl: # empty
citation: # empyty
slidesurl: # empyty
---

**关键词**
- 知识密集 / Knowledge‑intensive  
- 配对与交错 / Paired & Interleaved  
- 多模态文档 / Multimodal Documents  
- 多模态大模型 / Large Multimodal Models (LMMs)  
- 数据集构建 / Dataset Construction  
- PIN‑14M 数据集  
- 可扩展性 / Scalability  
- 图像‑文本对齐 / Image‑Text Alignment

论文地址：[https://arxiv.org/abs/2406.13923](https://arxiv.org/abs/2406.13923)

## 背景（Background）
近年来，大型多模态模型（LMMs）在处理复合知识任务时依赖海量数据进行训练，但仍面临视觉理解和推理能力不足的问题。这些模型在解析复杂视觉信息和推断多模态关系时频繁出错。为应对这一挑战，研究者提出了 PIN 数据格式，旨在增强多模态模型在“知识感知”方面的能力。

<p align=center>
	<img src="/images/papers/2024-pin/pin-1.png" width="100%">
</p>

## 方法（Method）

<p align=center>
	<img src="/images/papers/2024-pin/pin-2.png" width="100%">
</p>

论文提出了三大设计原则来驱动 PIN 数据格式的发展：

1. **知识密集性（Knowledge‑intensive）**  
   通过结合结构化 markdown 文件与整页图像，保证每个样本包含丰富知识内容。

2. **可扩展性（Scalability）**  
   PIN 格式兼容多种数据源，便于在现有数据集上插入转换和扩展。

3. **多训练策略支持**  
   该格式适用于多种训练方式：图像-文本对齐（Contrastive Learning）、掩码语言模型（MLM）、跨模态生成等。

**数据处理流程**：
- 从网页、学术期刊（如 arXiv, PMC）采集原始 PDF/XML 文档，并转换为 markdown 文本和整页图像。
- 应用工具（pdf2image、s2orc-doc2json）生成页面图像和结构化文本。
- 进行质量控制，确保图像-文本对应一致，清除噪声内容。

最终生成了**PIN-14M**，一个包含约 1 400 万样本的中英文多模态数据集。

<p align=center>
	<img src="/images/papers/2024-pin/pin-3.png" width="100%">
</p>

## 实验（Experiments）
论文的实验内容主要包括：

- **数据统计与主题分析**：使用主题建模方法（如 LDA）分析数据集的主题分布，揭示样本语义和视觉主题的多样性 :contentReference[oaicite:4]{index=4}。
- **训练策略探索**：提出可用于 PIN-14M 的多种训练策略，包括：
  - 图像–文本对比学习（Contrastive Learning）
  - 图像–文本匹配（ITM）
  - 掩码语言建模（MLM）以及多模态下一标记预测
  - 跨模态插入式序列生成
- **初步模型训练评估**：虽然当前主要作为技术报告披露，仍展示了使用 PIN 格式训练的 LMM 在知识推理和视觉理解任务上的潜力提升。

## 结果（Results）
- PIN-14M 数据集在知识密度、内容多样性和中英文覆盖方面表现优异。
- 作者指出，未来将在性能评估、更多训练范式和数据扩展方面持续推进。
